{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C26NIhWyIWuL"
      },
      "source": [
        "# Problem Set 1\n",
        "\n",
        "As noted in the instructions in the README (if you haven't read the README yet, please go look at it!), you can do this problem set in a stand-alone Python program or in this notebook. I can't provide any assistance installing libraries and making sure that you are using the right version of Python on your own computer so if you are not comfortable with that sort of thing, you should use Colab.\n",
        "\n",
        "If you choose to use Colab, follow the same instructions you have been following all along: (1) rename the notebook as `Lastname_Firstname_PS1.ipynb`; (2) download and push the notebook to your GitHub repo, and (3) share your notebook with me and the TAs.\n",
        "\n",
        "This problem set is due Friday, February 21, at 11:59pm EST. You still have to answer all the questions in the README, so please read the README. You will fail the problem set if you skip reading the README since that is where the instructions and questions are.\n",
        "\n",
        "## 1. Getting the data\n",
        "You can run the following code block to get the data. **Do not try to commit to or fork this repo or really anything else other than cloing this repo.** It's for you to get the data easily without worrying about mounting your Google Drive, which can be confusing for students.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOhizfDkIQft",
        "outputId": "f47c2a66-4739-47b5-8deb-89428b9f9054"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ps1'...\n",
            "remote: Enumerating objects: 2043, done.\u001b[K\n",
            "remote: Counting objects: 100% (2043/2043), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2041/2041), done.\u001b[K\n",
            "remote: Total 2043 (delta 13), reused 2009 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (2043/2043), 3.57 MiB | 5.91 MiB/s, done.\n",
            "Resolving deltas: 100% (13/13), done.\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/CSCI-3349-S25/ps1.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFVrMhYqLNDa"
      },
      "source": [
        "You can see now that you have a `ps1` directory with a few different directories in it: `pos`, `neg`, and `test`. Enter some unix code below to make sure it looks correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0zwxHHfJS4x",
        "outputId": "b9f45f5a-a343-493c-aafd-7e85a4e4e52c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ps1  sample_data\n",
            "nb.py  neg  pos  README.md  test\n"
          ]
        }
      ],
      "source": [
        "# unix commands to make sure you have ps1\n",
        "# and that inside ps1 are pos, neg, and test\n",
        "! ls\n",
        "! ls ps1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTyNC-xELeX9"
      },
      "source": [
        "## 2. Setting some global variables\n",
        "\n",
        "Run the code below to set the global variables. Read the comments to understand what they are for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eaJvu-n5JV89"
      },
      "outputs": [],
      "source": [
        "from nltk import FreqDist\n",
        "import glob\n",
        "import math\n",
        "import re\n",
        "from textblob import TextBlob\n",
        "\n",
        "########################\n",
        "### GLOBAL VARIABLES ###\n",
        "########################\n",
        "\n",
        "## hand-crafted list of stop words\n",
        "stops = {\"(\", \")\", \"--\", \"*\", \":\", \"-\", \"may\", \"though\", \";\",\n",
        "         \"thing\", \"things\", \"'d\", \"'ll\", \"'m\", \"'ve\", \"'t\", \"'s\",\n",
        "         \"'re\", \"a\", \"about\", \"above\", \"after\", \"again\", \"against\",\n",
        "         \"ain\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"aren't\",\n",
        "         \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\",\n",
        "         \"between\", \"both\", \"but\", \"by\", \"can\", \"could\", \"couldn't\",\n",
        "         \"did\", \"didn't\", \"do\", \"does\", \"doesn't\", \"doing\", \"don't\", \"down\",\n",
        "         \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"hadn't\",\n",
        "         \"has\", \"hasn't\", \"have\", \"haven't\", \"having\", \"he\", \"he'd\", \"he'll\",\n",
        "         \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\",\n",
        "         \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\",\n",
        "         \"into\", \"is\", \"isn't\", \"it\", \"it's\", \"its\", \"itself\", \"just\", \"let's\",\n",
        "         \"me\", \"mightn't\", \"more\", \"most\", \"mustn't\", \"my\", \"myself\",\n",
        "         \"needn't\", \"no\", \"nor\", \"not\", \"now\", \"o\", \"of\", \"off\", \"on\", \"once\",\n",
        "         \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\",\n",
        "         \"over\", \"own\", \"same\", \"shan't\", \"she\", \"she'd\", \"she'll\", \"she's\",\n",
        "         \"should\", \"should've\", \"shouldn't\", \"so\", \"some\", \"such\", \"t\", \"than\",\n",
        "         \"that\", \"that'll\", \"that's\", \"the\", \"their\", \"theirs\", \"them\",\n",
        "         \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\",\n",
        "         \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\",\n",
        "         \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"wasn't\", \"we\", \"we'd\",\n",
        "         \"we'll\", \"we're\", \"we've\", \"were\", \"weren't\", \"what\", \"what's\",\n",
        "         \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\",\n",
        "         \"who's\", \"whom\", \"why\", \"why's\", \"will\", \"with\", \"won't\", \"would\",\n",
        "         \"wouldn\", \"wouldn't\", \"y'all\", \"you\", \"you'd\", \"you'll\", \"you're\",\n",
        "         \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\", \",\", \".\", \"!\",\n",
        "         \"?\", \"'\", '\"', \"I\", \"i\"}\n",
        "\n",
        "# these store the words used in positive and negative reviews\n",
        "# these will be populated in read_in_training_data()\n",
        "poswords = []\n",
        "negwords = []\n",
        "\n",
        "# these store the NB probability for each word in positive reviews (\"positive word\")\n",
        "# and each word in negative reviews (\"negative word\")\n",
        "# these will be populated in calculate_nb_probabilities()\n",
        "poswordprobs = {}\n",
        "negwordprobs = {}\n",
        "\n",
        "# these store the smoothed NB probability for each word\n",
        "# these will be populated in calculate_smooth_nb_probabilities()\n",
        "smooth_poswordprobs = {}\n",
        "smooth_negwordprobs = {}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrVFbzH4MHlJ"
      },
      "source": [
        "## 3. Read in the data\n",
        "The code below reads in the data. You do not need to modify this code. Read the comments to understand what it's doing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UapeN8XLMQCE"
      },
      "outputs": [],
      "source": [
        "#################################\n",
        "### FUNCTION TO READ IN DATA ###\n",
        "#################################\n",
        "# You don't need to modify this.\n",
        "# This returns the lists of positive words\n",
        "# and negative words from the training data.\n",
        "\n",
        "def read_in_training_data():\n",
        "\n",
        "\n",
        "    ## Read in all positive reviews\n",
        "    ## We create a set of unique words for each review.\n",
        "    ## We then add that set of words as a list to the master list of positive words.\n",
        "    positivewords = []\n",
        "    allpos = glob.glob(\"ps1/pos/*\")\n",
        "    for filename in allpos:\n",
        "        f = open(filename)\n",
        "        thesewords = set()\n",
        "        for line in f:\n",
        "            words = line.rstrip().split()\n",
        "            for w in words:\n",
        "                if w not in stops:\n",
        "                    thesewords.add(w)\n",
        "        f.close()\n",
        "        positivewords.extend(list(thesewords))\n",
        "\n",
        "    print(len(positivewords), \"positive tokens found!\")\n",
        "    print(len(set(positivewords)), \"positive types found!\")\n",
        "\n",
        "\n",
        "    ## Read in all negative reviews\n",
        "    ## We create a set of unique words for each review.\n",
        "    ## We then add that set of words as a list to the master list of negative words.\n",
        "    negativewords = []\n",
        "    allneg = glob.glob(\"ps1/neg/*\")\n",
        "    for filename in allneg:\n",
        "        f = open(filename)\n",
        "        thesewords = set()\n",
        "        for line in f:\n",
        "            words = line.rstrip().split()\n",
        "            for w in words:\n",
        "                if w not in stops:\n",
        "                    thesewords.add(w)\n",
        "        f.close()\n",
        "        negativewords.extend(list(thesewords))\n",
        "\n",
        "    print(len(negativewords), \"negative tokens found!\")\n",
        "    print(len(set(negativewords)), \"negative types found!\")\n",
        "    return(positivewords, negativewords)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a9THjeSMtc1"
      },
      "source": [
        "## 4. Writing the functions\n",
        "Each code block below contains a function for predicting the sentiment of a movie review (positive or negative) in a different way. For each code block, you need to fill in code where it say `YOUR PART X CODE STARTS HERE`.\n",
        "\n",
        "You can run each of these code cells one by one without changing anything. At the moment they don't do anything, so in the very last code block, you will get 50\\% accuracy for everything.\n",
        "\n",
        "Then you should fill in each section as indicated in the comments. You should be able to complete the problem set just be reading the instructions in the README and following the comments below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vmCxehtgMQvO"
      },
      "outputs": [],
      "source": [
        "## FUNCTION USING USER-DEFINED WORDS TO PREDICT SENTIMENT\n",
        "# You just need to fill in your own keywords below.\n",
        "\n",
        "def user_defined_keywords(reviewwords):\n",
        "\n",
        "\n",
        "    #########################################\n",
        "    ##### YOUR PART A CODE STARTS HERE ######\n",
        "    #########################################\n",
        "\n",
        "    # enter your keywords in the lists below\n",
        "    positive_keywords = [\"real\", \"beautiful\", \"brilliant\", \"glamorous\", \"exceptional\", \"great\", \"sweet\", \"nice\", \"craft\", \"engrossed\"]\n",
        "    negative_keywords = [\"not\", \"bad\", \"worse\", \"*\", \"dumb\", \"grainy\", \"no\", \"sucked\", \"inept\", \"damn\"]\n",
        "\n",
        "    #########################################\n",
        "    ##### YOUR PART A CODE ENDS HERE ########\n",
        "    #########################################\n",
        "\n",
        "\n",
        "    # If there are more positive than negative keywords,\n",
        "    # return \"pos\". Otherwise, return \"neg\".\n",
        "\n",
        "    sentiment = 0\n",
        "    for w in reviewwords:\n",
        "        if w in positive_keywords:\n",
        "            sentiment += 1\n",
        "        if w in negative_keywords:\n",
        "            sentiment -=1\n",
        "\n",
        "    if sentiment > 0:\n",
        "        return \"pos\"\n",
        "\n",
        "    return \"neg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bFRf06smMX21"
      },
      "outputs": [],
      "source": [
        "## FUNCTION TO CALCULATE NAIVE BAYES PROBABILITIES\n",
        "# You will be writing most of this function.\n",
        "def calculate_nb_probabilities():\n",
        "\n",
        "    ## GOAL: Populate these two dicts, where each\n",
        "    ##      key = word from poswords or negwords (created for you above)\n",
        "    ##      value = NB probability for that word in that class (calculated by you here)\n",
        "\n",
        "    poswordprobs = {}\n",
        "    negwordprobs = {}\n",
        "\n",
        "    #########################################\n",
        "    ##### YOUR PART B CODE STARTS HERE ######\n",
        "    #########################################\n",
        "\n",
        "    ## Create a FreqDist for poswords below.\n",
        "    poswords_freq = FreqDist(poswords)\n",
        "\n",
        "    ## Create a FreqDist for negwords below.\n",
        "    negwords_freq = FreqDist(negwords)\n",
        "\n",
        "    ## Loop through your poswords FreqDist, and calculate the\n",
        "    ## probability of each word in the positive class, like this:\n",
        "    ## P(word|pos) = count(word) / total number of positive tokens\n",
        "    ## where count(word) is what you get from the FreqDist for poswords.\n",
        "    ## Store the results in poswordprobs.\n",
        "    ## USE LOGS!!!\n",
        "\n",
        "    for word, count in poswords_freq.items():\n",
        "        poswordprobs[word] = math.log(count / len(poswords))\n",
        "\n",
        "\n",
        "    ## Now, loop through your negwords FreqDist, and calculate the\n",
        "    ## probability of each word in the negative class, like this:\n",
        "    ## P(word|neg) = count(word) / total number of negative tokens\n",
        "    ## where count(word) is what you get from the FreqDist for negwords.\n",
        "    ## Store the results in negwordprobs.\n",
        "    ## USE LOGS!!!\n",
        "\n",
        "    for word, count in negwords_freq.items():\n",
        "        negwordprobs[word] = math.log(count / len(negwords))\n",
        "\n",
        "\n",
        "    #########################################\n",
        "    ##### YOUR PART B CODE ENDS HERE ########\n",
        "    #########################################\n",
        "\n",
        "    return (poswordprobs, negwordprobs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g--GQytXkiq8",
        "outputId": "fad92a2b-8d98-4259-bddd-941eea3e6e2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ps1/test/pos-cv972_26417.txt\n",
            "ps1/test/neg-cv936_17473.txt\n",
            "ps1/test/neg-cv935_24977.txt\n",
            "ps1/test/neg-cv903_18981.txt\n",
            "ps1/test/pos-cv998_14111.txt\n",
            "ps1/test/neg-cv962_9813.txt\n",
            "ps1/test/neg-cv907_3193.txt\n",
            "ps1/test/neg-cv956_12547.txt\n",
            "ps1/test/pos-cv914_28742.txt\n",
            "ps1/test/pos-cv997_5046.txt\n",
            "ps1/test/neg-cv941_10718.txt\n",
            "ps1/test/neg-cv991_19973.txt\n",
            "ps1/test/pos-cv988_18740.txt\n",
            "ps1/test/neg-cv952_26375.txt\n",
            "ps1/test/pos-cv917_29715.txt\n",
            "ps1/test/neg-cv998_15691.txt\n",
            "ps1/test/pos-cv971_10874.txt\n",
            "ps1/test/pos-cv944_13521.txt\n",
            "ps1/test/neg-cv977_4776.txt\n",
            "ps1/test/pos-cv955_25001.txt\n",
            "ps1/test/pos-cv909_9960.txt\n",
            "ps1/test/neg-cv986_15092.txt\n",
            "ps1/test/pos-cv942_17082.txt\n",
            "ps1/test/neg-cv924_29397.txt\n",
            "ps1/test/pos-cv908_16009.txt\n",
            "ps1/test/neg-cv950_13478.txt\n",
            "ps1/test/pos-cv925_8969.txt\n",
            "ps1/test/pos-cv990_11591.txt\n",
            "ps1/test/neg-cv921_13988.txt\n",
            "ps1/test/neg-cv945_13012.txt\n",
            "ps1/test/pos-cv982_21103.txt\n",
            "ps1/test/pos-cv979_18921.txt\n",
            "ps1/test/neg-cv951_11816.txt\n",
            "ps1/test/pos-cv907_3541.txt\n",
            "ps1/test/pos-cv918_2693.txt\n",
            "ps1/test/pos-cv937_9811.txt\n",
            "ps1/test/neg-cv926_18471.txt\n",
            "ps1/test/neg-cv978_22192.txt\n",
            "ps1/test/pos-cv970_18450.txt\n",
            "ps1/test/pos-cv978_20929.txt\n",
            "ps1/test/neg-cv980_11851.txt\n",
            "ps1/test/neg-cv937_9816.txt\n",
            "ps1/test/neg-cv970_19532.txt\n",
            "ps1/test/pos-cv904_24353.txt\n",
            "ps1/test/neg-cv982_22209.txt\n",
            "ps1/test/neg-cv975_11920.txt\n",
            "ps1/test/pos-cv958_12162.txt\n",
            "ps1/test/neg-cv984_14006.txt\n",
            "ps1/test/pos-cv984_12767.txt\n",
            "ps1/test/neg-cv946_20084.txt\n",
            "ps1/test/neg-cv981_16679.txt\n",
            "ps1/test/neg-cv963_7208.txt\n",
            "ps1/test/neg-cv928_9478.txt\n",
            "ps1/test/neg-cv955_26154.txt\n",
            "ps1/test/pos-cv946_18658.txt\n",
            "ps1/test/pos-cv924_29540.txt\n",
            "ps1/test/pos-cv926_17059.txt\n",
            "ps1/test/pos-cv932_13401.txt\n",
            "ps1/test/neg-cv965_26688.txt\n",
            "ps1/test/neg-cv922_10185.txt\n",
            "ps1/test/neg-cv960_28877.txt\n",
            "ps1/test/pos-cv952_25240.txt\n",
            "ps1/test/pos-cv934_19027.txt\n",
            "ps1/test/pos-cv983_22928.txt\n",
            "ps1/test/pos-cv966_28832.txt\n",
            "ps1/test/neg-cv966_28671.txt\n",
            "ps1/test/neg-cv905_28965.txt\n",
            "ps1/test/neg-cv972_26837.txt\n",
            "ps1/test/pos-cv921_12747.txt\n",
            "ps1/test/pos-cv959_14611.txt\n",
            "ps1/test/pos-cv947_10601.txt\n",
            "ps1/test/pos-cv989_15824.txt\n",
            "ps1/test/neg-cv985_5964.txt\n",
            "ps1/test/neg-cv911_21695.txt\n",
            "ps1/test/pos-cv995_21821.txt\n",
            "ps1/test/pos-cv987_6965.txt\n",
            "ps1/test/neg-cv939_11247.txt\n",
            "ps1/test/pos-cv901_11017.txt\n"
          ]
        }
      ],
      "source": [
        " #find misclassifications\n",
        " # read in the test reviews\n",
        "testdata = glob.glob(\"ps1/test/*\")\n",
        "for filename in testdata:\n",
        "    wholereview = \"\"\n",
        "    reviewwords = []\n",
        "    with open(filename, encoding='utf8') as f:\n",
        "        wholereview = f.read().rstrip()\n",
        "    words = set(wholereview.split())\n",
        "    for w in words:\n",
        "        if w not in stops:\n",
        "            reviewwords.append(w)\n",
        "\n",
        "        # read the file name of the file to determine if its pos or neg\n",
        "    filepolarity = re.sub(r\"^.*?(pos|neg)-.*?$\", r\"\\1\", filename)\n",
        "\n",
        "        # apply each classifier to that review, and check to see it's correct\n",
        "    if filepolarity != user_defined_keywords(reviewwords):\n",
        "      print(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6o16VkrmRmKQ"
      },
      "outputs": [],
      "source": [
        "## FUNCTION USING NAIVE BAYES PROBS TO PREDICT SENTIMENT\n",
        "# You don't need to modify this method, but it relies\n",
        "# on the code you  wrote above.\n",
        "\n",
        "def naive_bayes(reviewwords):\n",
        "\n",
        "    # default probability for unseen words\n",
        "    defaultprob = math.log(0.0000000000001)\n",
        "\n",
        "    ### POSITIVE SCORE\n",
        "    posscore = poswordprobs.get(reviewwords[0], defaultprob)\n",
        "    for i in range(1, len(reviewwords)):\n",
        "        posscore += poswordprobs.get(reviewwords[i], defaultprob)\n",
        "\n",
        "    ### CALCULATE NEGATIVE SCORE\n",
        "    negscore = negwordprobs.get(reviewwords[0], defaultprob)\n",
        "    for i in range(1, len(reviewwords)):\n",
        "        negscore += negwordprobs.get(reviewwords[i], defaultprob)\n",
        "\n",
        "    if (posscore - negscore) >  0:\n",
        "        return \"pos\"\n",
        "\n",
        "    return \"neg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0ya6V8fsRogZ"
      },
      "outputs": [],
      "source": [
        "## FUNCTION TO CALCULATE SMOOTHED NAIVE BAYES PROBABILITIES\n",
        "# You will write most of this function.\n",
        "def calculate_smooth_nb_probabilities():\n",
        "\n",
        "    smooth_poswordprobs = {}\n",
        "    smooth_negwordprobs = {}\n",
        "\n",
        "    #########################################\n",
        "    ##### YOUR PART C CODE STARTS HERE ######\n",
        "    #########################################\n",
        "\n",
        "    # Populate the above dictionaries just as you did in the unsmoothed\n",
        "    # version, but use +1 smoothing so that you can handle unseen words.\n",
        "\n",
        "    # +1 smoothing: when calculating the probabilities,\n",
        "    # add 1 to every count found in the FreqDist for each class.\n",
        "    # Divide the count by the number of types...\n",
        "    #     *plus* the number of tokens for that class...\n",
        "    #     *plus* 1 (for the count of the unseen word)\n",
        "\n",
        "    # Don't forget to use logs.\n",
        "\n",
        "    poswords_freq = FreqDist(poswords)\n",
        "    negwords_freq = FreqDist(negwords)\n",
        "\n",
        "    for word, count in poswords_freq.items():\n",
        "        smooth_poswordprobs[word] = math.log((count + 1) / (len(poswords) + 1))\n",
        "\n",
        "    for word, count in negwords_freq.items():\n",
        "        smooth_negwordprobs[word] = math.log((count + 1) / (len(negwords) + 1))\n",
        "\n",
        "    return (smooth_poswordprobs, smooth_negwordprobs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "omegM9c0RrXK"
      },
      "outputs": [],
      "source": [
        "## FUNCTION USING SMOOTHED NAIVE BAYES PROBS TO PREDICT SENTIMENT\n",
        "# You will write most of this function.\n",
        "def smooth_naive_bayes(reviewwords):\n",
        "\n",
        "    # These are placeholders that allow the code to run.\n",
        "    # You will calculate posscore and negscore below.\n",
        "    posscore = 0\n",
        "    negscore = 0\n",
        "\n",
        "    # Adapt the code from naive_bayes() above to work here.\n",
        "    # Use the smoothed probabilities you created above.\n",
        "\n",
        "    # Do not forget to create a separate defaultprob for\n",
        "    # unseen words for the two classes, as follows.\n",
        "\n",
        "    # The defaultprob for each class should be\n",
        "    # the log of:\n",
        "    # 1 (the count of the unseen word) divided by...\n",
        "    #    the number of types in that class...\n",
        "    #    *plus* the number of tokens in that class...\n",
        "    #    *plus* 1\n",
        "\n",
        "    defaultprob = math.log(1 / (len(poswords) + 1))\n",
        "    posscore = smooth_poswordprobs.get(reviewwords[0], defaultprob)\n",
        "    for i in range(1, len(reviewwords)):\n",
        "        posscore += smooth_poswordprobs.get(reviewwords[i], defaultprob)\n",
        "\n",
        "    negscore = smooth_negwordprobs.get(reviewwords[0], defaultprob)\n",
        "    for i in range(1, len(reviewwords)):\n",
        "        negscore += smooth_negwordprobs.get(reviewwords[i], defaultprob)\n",
        "\n",
        "    #########################################\n",
        "    ##### YOUR PART C CODE ENDS HERE ########\n",
        "    #########################################\n",
        "\n",
        "\n",
        "    if (posscore - negscore) >  0:\n",
        "        return \"pos\"\n",
        "\n",
        "    return \"neg\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testdata = glob.glob(\"ps1/test/*\")\n",
        "for filename in testdata:\n",
        "    wholereview = \"\"\n",
        "    reviewwords = []\n",
        "    with open(filename, encoding='utf8') as f:\n",
        "        wholereview = f.read().rstrip()\n",
        "    words = set(wholereview.split())\n",
        "    for w in words:\n",
        "        if w not in stops:\n",
        "            reviewwords.append(w)\n",
        "\n",
        "        # read the file name of the file to determine if its pos or neg\n",
        "    filepolarity = re.sub(r\"^.*?(pos|neg)-.*?$\", r\"\\1\", filename)\n",
        "\n",
        "        # apply each classifier to that review, and check to see it's correct\n",
        "    if filepolarity != naive_bayes (reviewwords) and filepolarity == smooth_naive_bayes(reviewwords):\n",
        "        print(filename)"
      ],
      "metadata": {
        "id": "8Jb7JBqTRiL6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: #Q8: Find three reviews that were still classified incorrectly, even with smoothing. Write them out here, and then comment on why you think they might have been challenging for the classifiers.\n",
        "\n",
        "# Find three reviews that were still classified incorrectly, even with smoothing.\n",
        "# The provided code already identifies misclassified reviews.  We just need to select three.\n",
        "\n",
        "\n",
        "# Example output (replace with actual misclassified reviews):\n",
        "# ps1/test/neg-23.txt\n",
        "# ps1/test/pos-11.txt\n",
        "# ps1/test/neg-42.txt\n",
        "\n",
        "\n",
        "# Potential reasons for misclassification:\n",
        "\n",
        "# 1. Sarcasm and irony:  Classifiers struggle with sarcasm, where positive words are used to express negative sentiments or vice versa.\n",
        "#    A review might contain many positive words but be highly critical due to sarcasm.  The classifier would be tricked into predicting a positive sentiment.\n",
        "\n",
        "# 2. Domain-specific language:  Movie reviews may contain slang, jargon, or abbreviations that the classifier hasn't encountered during training.\n",
        "#   The model may misinterpret the meaning of these terms, leading to incorrect classifications.\n",
        "\n",
        "# 3. Negation handling: A word like \"not\" can change the meaning of subsequent words, but classifiers might miss the context around a negation.\n",
        "#    For instance, \"not bad\" means something different from just \"bad\".\n",
        "\n",
        "# 4. Ambiguity of words: Some words have multiple meanings in different contexts. The model might not correctly identify the meaning of these words based on the review content.\n",
        "\n",
        "# 5. Limited training data: If the training data does not include examples of reviews that use similar wording patterns to the misclassified reviews, the model may not perform well.\n",
        "\n",
        "\n",
        "# To analyze the specific reasons for misclassification in the three example reviews above, you should inspect the reviews themselves and compare their content with the keywords used in the classifier. Look for potential cases of sarcasm, uncommon words or phrases, negations, or ambiguous words.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "e-22QYvFWJYF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "rpRkDnSqRto3"
      },
      "outputs": [],
      "source": [
        "## FUNCTION FOR APPLYING TEXTBLOB's SENTIMENT TOOL TO A REVIEW\n",
        "def calculate_textblob(review):\n",
        "\n",
        "    #########################################\n",
        "    ##### YOUR PART D CODE STARTS HERE ######\n",
        "    #########################################\n",
        "\n",
        "    # First, tead the documentation for the textblob sentiment analysis here:\n",
        "    # https://textblob.readthedocs.io/en/dev/quickstart.html#sentiment-analysis\n",
        "    # You will not be able to do this without reading the documentation.\n",
        "\n",
        "    # Brief instructions:\n",
        "    # Create a TextBlob object.\n",
        "    # Populate it to with the review (as a string not as a list of words).\n",
        "    # Get the first element of its sentiment variable.\n",
        "    # If it's more than the threshold 0, return pos. Otherwise return neg.\n",
        "\n",
        "    threshold = 0.09\n",
        "    blob = TextBlob(\" \".join(review))\n",
        "    if blob.sentiment.polarity > threshold:\n",
        "        return \"pos\"\n",
        "\n",
        "    #########################################\n",
        "    ##### YOUR PART D CODE ENDS HERE ########\n",
        "    #########################################\n",
        "\n",
        "    return \"neg\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9kEWD8BARxX5"
      },
      "outputs": [],
      "source": [
        "## FUNCTION FOR CALCULATING THE ACCURACY OF YOUR MODELS\n",
        "# You do not need to modify this code.\n",
        "\n",
        "def calculate_accuracy():\n",
        "    keywordscorrect = 0\n",
        "    nbcorrect = 0\n",
        "    smnbcorrect = 0\n",
        "    tbcorrect = 0\n",
        "    affcorrect = 0\n",
        "\n",
        "    # read in the test reviews\n",
        "    testdata = glob.glob(\"ps1/test/*\")\n",
        "    for filename in testdata:\n",
        "        wholereview = \"\"\n",
        "        reviewwords = []\n",
        "        with open(filename, encoding='utf8') as f:\n",
        "            wholereview = f.read().rstrip()\n",
        "        words = set(wholereview.split())\n",
        "        for w in words:\n",
        "            if w not in stops:\n",
        "                reviewwords.append(w)\n",
        "\n",
        "        # read the file name of the file to determine if its pos or neg\n",
        "        filepolarity = re.sub(r\"^.*?(pos|neg)-.*?$\", r\"\\1\", filename)\n",
        "\n",
        "        # apply each classifier to that review, and check to see it's correct\n",
        "        if filepolarity == user_defined_keywords(reviewwords):\n",
        "            keywordscorrect += 1\n",
        "\n",
        "        if filepolarity == naive_bayes(reviewwords):\n",
        "            nbcorrect += 1\n",
        "\n",
        "        if filepolarity == smooth_naive_bayes(reviewwords):\n",
        "            smnbcorrect += 1\n",
        "\n",
        "        if filepolarity == calculate_textblob(reviewwords):\n",
        "            tbcorrect += 1\n",
        "\n",
        "    # report the accuracy of each classifier\n",
        "    print(\"User keyword accuracy: \", (keywordscorrect/200))\n",
        "    print(\"Naive Bayes accuracy: \", (nbcorrect/200))\n",
        "    print(\"Snoothed Naive Bayes accuracy: \", (smnbcorrect/200))\n",
        "    print(\"TextBlob accuracy: \", (tbcorrect/200))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zPXlmQNgTDU"
      },
      "source": [
        "## 5. Evaluate!\n",
        "\n",
        "Again, you do not need to modify this. This will work without changing anything, and will continue to work as long as your functions do what they are supposed to do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehPLuYlDRz4Q",
        "outputId": "d623e532-4dac-4db6-c7a1-5040c1465517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "251965 positive tokens found!\n",
            "34422 positive types found!\n",
            "228452 negative tokens found!\n",
            "32227 negative types found!\n",
            "User keyword accuracy:  0.61\n",
            "Naive Bayes accuracy:  0.725\n",
            "Snoothed Naive Bayes accuracy:  0.845\n",
            "TextBlob accuracy:  0.74\n"
          ]
        }
      ],
      "source": [
        "#####################\n",
        "### RUN ALL TESTS ###\n",
        "#####################\n",
        "\n",
        "# You do not need to modify this code.\n",
        "\n",
        "# read in the training data to get all the positive and negative words\n",
        "poswords, negwords = read_in_training_data()\n",
        "\n",
        "# calculate the naive bayes probabilities\n",
        "poswordprobs, negwordprobs = calculate_nb_probabilities()\n",
        "\n",
        "# calculate smoothed naive bayes probabilities\n",
        "smooth_poswordprobs, smooth_negwordprobs = calculate_smooth_nb_probabilities()\n",
        "\n",
        "# calculate the accuracy of all approaches\n",
        "calculate_accuracy()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}